{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from CVUSA_dataset import CVUSA_dataset_cropped, CVUSA_Dataset_Eval\n",
    "from custom_models import ResNet, VIT, CLIP_model\n",
    "from losses import SoftTripletBiLoss\n",
    "from train import train\n",
    "from eval import predict, accuracy, calculate_scores\n",
    "from pytorch_metric_learning import losses as LS\n",
    "from helper_func import get_rand_id, hyparam_info, save_losses\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_path = '/media/fahimul/2B721C03261BDC8D/Research/datasets/CVUSA' #don't include the / at the end\n",
    "# data_path = '/home/fa947945/datasets/CVUSA_Cropped/CVUSA' #don't include the / at the end\n",
    "data_path = '/data/Research/Dataset/CVUSA_Cropped/CVUSA' #don't include the / at the end\n",
    "\n",
    "train_data= pd.read_csv(f'{data_path}/splits/train-19zl.csv')\n",
    "val_data= pd.read_csv(f'{data_path}/splits/val-19zl.csv')\n",
    "\n",
    "# df_loss = pd.DataFrame(columns=['Loss'])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "# transform = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "train_ds = CVUSA_dataset_cropped(df = train_data, path=data_path, transform=transform)\n",
    "val_que = CVUSA_Dataset_Eval(data_folder=data_path, split='train', img_type='query', transforms=transform)\n",
    "val_ref = CVUSA_Dataset_Eval(data_folder=data_path, split='train', img_type='reference', transforms=transform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    embed_dim = 256\n",
    "    lr = 0.001\n",
    "    batch_size = 64\n",
    "    epochs = 500\n",
    "    expID = get_rand_id()\n",
    "    loss_margin = 1\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_que = DataLoader(val_que, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_ref = DataLoader(val_ref, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # model = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model_r = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model_q = ResNet(emb_dim=embed_dim).to(device)\n",
    "\n",
    "    # model = VIT().to(device)\n",
    "    model = CLIP_model()\n",
    "    \n",
    "    # criterion = TripletLoss(margin=loss_margin)\n",
    "    # criterion = nn.TripletMarginLoss(margin=0.5)\n",
    "    criterion = SoftTripletBiLoss()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name)\n",
    "    optimizer = optim.Adam(parameters, lr=lr)\n",
    "    # optimizer = optim.AdamW(parameters, lr=lr)\n",
    "    # optimizer = optim.SGD(parameters, lr=lr)\n",
    "\n",
    "    \n",
    "    \n",
    "    hyparam_info(emb_dim=embed_dim, loss_id=expID, ln_rate=lr, batch=batch_size, epc=epochs, ls_mrgn=loss_margin, trn_sz=train_data.shape[0], mdl_nm=model.modelName)\n",
    "\n",
    "    print(\"Training Start\")\n",
    "    all_loses = train(model, criterion, optimizer, train_loader, num_epochs=epochs, dev=device)\n",
    "    df_loss = pd.DataFrame({'Loss': all_loses})\n",
    "    df_loss.to_csv(f'losses/losses_{expID}.csv')\n",
    "\n",
    "\n",
    "    print(\"\\nExtract Features:\")\n",
    "    query_features, query_labels = predict(model=model, dataloader=val_loader_que, dev=device, isQuery=True)\n",
    "    reference_features, reference_labels = predict(model = model, dataloader=val_loader_ref, dev=device, isQuery=False) \n",
    "    \n",
    "    print(query_labels)\n",
    "\n",
    "\n",
    "    print(\"Compute Scores:\")\n",
    "    # r1 =  calculate_scores(query_features, reference_features, query_labels, reference_labels, step_size=1000, ranks=[1, 5, 10])\n",
    "    r1 =  accuracy(query_features=query_features, reference_features=reference_features, query_labels=query_labels, topk=[1, 5, 10])\n",
    "    \n",
    "    save_losses(df=df_loss, \n",
    "                emb_dim=embed_dim, \n",
    "                loss_id=expID, \n",
    "                ln_rate=lr, \n",
    "                batch=batch_size, \n",
    "                epc=epochs, \n",
    "                ls_mrgn=loss_margin, \n",
    "                trn_sz=train_data.shape[0],\n",
    "                mdl_nm=model.modelName,\n",
    "                rslt=r1)\n",
    "\n",
    "\n",
    "    print(r1) \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# from torch.cuda.amp import autocast\n",
    "import numpy as np\n",
    "from CVUSA_dataset import CVUSA_dataset_cropped, CVUSA_Dataset_Eval\n",
    "# from CVUSA_dataset import CVUSA_Dataset_Eval\n",
    "from custom_models import ResNet, VIT, CLIP_model\n",
    "from losses import SoftTripletBiLoss, InfoNCE\n",
    "from train import train\n",
    "from eval import predict, accuracy, calculate_scores\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "from pytorch_metric_learning import losses as LS\n",
    "from helper_func import get_rand_id, hyparam_info, save_losses\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_path = '/media/fahimul/2B721C03261BDC8D/Research/datasets/CVUSA' #don't include the / at the end\n",
    "# data_path = '/home/fa947945/datasets/CVUSA_Cropped/CVUSA' #don't include the / at the end\n",
    "data_path = '/data/Research/Dataset/CVUSA_Cropped/CVUSA' #don't include the / at the end\n",
    "\n",
    "train_data= pd.read_csv(f'{data_path}/splits/train-19zl.csv')\n",
    "val_data= pd.read_csv(f'{data_path}/splits/val-19zl.csv')\n",
    "\n",
    "# df_loss = pd.DataFrame(columns=['Loss'])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# transform = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "train_ds = CVUSA_dataset_cropped(df = train_data, path=data_path, transform=transform)\n",
    "val_que = CVUSA_Dataset_Eval(data_folder=data_path, split='val', img_type='query', transforms=transform)\n",
    "val_ref = CVUSA_Dataset_Eval(data_folder=data_path, split='val', img_type='reference', transforms=transform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    embed_dim = 1000\n",
    "    lr = 0.004\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    expID = get_rand_id()\n",
    "    loss_margin = 1\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_que = DataLoader(val_que, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_ref = DataLoader(val_ref, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # model = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model_r = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model_q = ResNet(emb_dim=embed_dim).to(device)\n",
    "\n",
    "    # model = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model = VIT().to(device)\n",
    "    model = CLIP_model(embed_dim=embed_dim)\n",
    "    \n",
    "    # torch.save(model, f'model_weights/model_st.pth')\n",
    "\n",
    "    # criterion = TripletLoss(margin=loss_margin)\n",
    "    # criterion = nn.TripletMarginLoss(margin=0.5)\n",
    "  \n",
    "    # criterion = SoftTripletBiLoss()\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    criterion = InfoNCE(loss_function=loss_fn,\n",
    "                            device=device,\n",
    "                            )\n",
    "\n",
    "\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name)\n",
    "    optimizer = optim.Adam(parameters, lr=lr)\n",
    "    # optimizer = optim.AdamW(parameters, lr=lr)\n",
    "    # optimizer = optim.SGD(parameters, lr=lr)\n",
    "\n",
    "    \n",
    "    \n",
    "    hyparam_info(emb_dim=embed_dim, loss_id=expID, ln_rate=lr, batch=batch_size, epc=epochs, ls_mrgn=loss_margin, trn_sz=train_data.shape[0], mdl_nm=model.modelName)\n",
    "\n",
    "    print(\"Training Start\")\n",
    "    all_loses = train(model, criterion, optimizer, train_loader, num_epochs=epochs, dev=device)\n",
    "    df_loss = pd.DataFrame({'Loss': all_loses})\n",
    "    df_loss.to_csv(f'losses/losses_{expID}.csv')\n",
    "\n",
    "\n",
    "    torch.save(model, f'model_weights/model_{expID}.pt')\n",
    "\n",
    "    print(\"\\nExtract Features:\")\n",
    "    query_features, query_labels = predict(model=model, dataloader=val_loader_que, dev=device, isQuery=True)\n",
    "    reference_features, reference_labels = predict(model = model, dataloader=val_loader_ref, dev=device, isQuery=False) \n",
    "    \n",
    "\n",
    "\n",
    "    print(\"Compute Scores:\")\n",
    "    # r1 =  calculate_scores(query_features, reference_features, query_labels, reference_labels, step_size=1000, ranks=[1, 5, 10])\n",
    "    r1 =  accuracy(query_features=query_features, reference_features=reference_features, query_labels=query_labels, topk=[1, 5, 10])\n",
    "    \n",
    "    save_losses(df=df_loss, \n",
    "                emb_dim=embed_dim, \n",
    "                loss_id=expID, \n",
    "                ln_rate=lr, \n",
    "                batch=batch_size, \n",
    "                epc=epochs, \n",
    "                ls_mrgn=loss_margin, \n",
    "                trn_sz=train_data.shape[0],\n",
    "                mdl_nm=model.modelName,\n",
    "                rslt=r1)\n",
    "\n",
    "\n",
    "    print(r1) \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# from torch.cuda.amp import autocast\n",
    "import numpy as np\n",
    "from CVUSA_dataset import CVUSA_dataset_cropped, CVUSA_Dataset_Eval\n",
    "# from CVUSA_dataset import CVUSA_Dataset_Eval\n",
    "from custom_models import ResNet, VIT, CLIP_model\n",
    "from losses import SoftTripletBiLoss, InfoNCE\n",
    "from train import train\n",
    "from eval import predict, accuracy, calculate_scores\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "from pytorch_metric_learning import losses as LS\n",
    "from helper_func import get_rand_id, hyparam_info, save_losses\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_path = '/media/fahimul/2B721C03261BDC8D/Research/datasets/CVUSA' #don't include the / at the end\n",
    "# data_path = '/home/fa947945/datasets/CVUSA_Cropped/CVUSA' #don't include the / at the end\n",
    "data_path = '/data/Research/Dataset/CVUSA_Cropped/CVUSA' #don't include the / at the end\n",
    "\n",
    "train_data= pd.read_csv(f'{data_path}/splits/train-19zl.csv')\n",
    "val_data= pd.read_csv(f'{data_path}/splits/val-19zl.csv')\n",
    "\n",
    "# df_loss = pd.DataFrame(columns=['Loss'])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# transform = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "train_ds = CVUSA_dataset_cropped(df = train_data, path=data_path, transform=transform)\n",
    "val_que = CVUSA_Dataset_Eval(data_folder=data_path, split='train', img_type='query', transforms=transform)\n",
    "val_ref = CVUSA_Dataset_Eval(data_folder=data_path, split='train', img_type='reference', transforms=transform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    embed_dim = 1000\n",
    "    lr = 0.005\n",
    "    batch_size = 64\n",
    "    epochs = 50\n",
    "    expID = get_rand_id()\n",
    "    loss_margin = 1\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_que = DataLoader(val_que, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_ref = DataLoader(val_ref, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # model = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model_r = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model_q = ResNet(emb_dim=embed_dim).to(device)\n",
    "\n",
    "    # model = ResNet()\n",
    "    # model = VIT().to(device)\n",
    "    model = CLIP_model(embed_dim=embed_dim)\n",
    "    \n",
    "    # torch.save(model, f'model_weights/model_st.pth')\n",
    "\n",
    "    # criterion = TripletLoss(margin=loss_margin)\n",
    "    # criterion = nn.TripletMarginLoss(margin=0.5)\n",
    "  \n",
    "    # criterion = SoftTripletBiLoss()\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    criterion = InfoNCE(loss_function=loss_fn,\n",
    "                            device=device,\n",
    "                            )\n",
    "\n",
    "\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name)\n",
    "    optimizer = optim.Adam(parameters, lr=lr)\n",
    "    # optimizer = optim.AdamW(parameters, lr=lr)\n",
    "    # optimizer = optim.SGD(parameters, lr=lr)\n",
    "\n",
    "    \n",
    "    \n",
    "    hyparam_info(emb_dim=embed_dim, loss_id=expID, ln_rate=lr, batch=batch_size, epc=epochs, ls_mrgn=loss_margin, trn_sz=train_data.shape[0], mdl_nm=model.modelName)\n",
    "\n",
    "    print(\"Training Start\")\n",
    "    all_loses = train(model, criterion, optimizer, train_loader, num_epochs=epochs, dev=device)\n",
    "    df_loss = pd.DataFrame({'Loss': all_loses})\n",
    "    df_loss.to_csv(f'losses/losses_{expID}.csv')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nExtract Features:\")\n",
    "    query_features, query_labels = predict(model=model, dataloader=val_loader_que, dev=device, isQuery=True)\n",
    "    reference_features, reference_labels = predict(model = model, dataloader=val_loader_ref, dev=device, isQuery=False) \n",
    "    \n",
    "\n",
    "\n",
    "    print(\"Compute Scores:\")\n",
    "    # r1 =  calculate_scores(query_features, reference_features, query_labels, reference_labels, step_size=1000, ranks=[1, 5, 10])\n",
    "    r1 =  accuracy(query_features=query_features, reference_features=reference_features, query_labels=query_labels, topk=[1, 5, 10])\n",
    "    \n",
    "    save_losses(df=df_loss, \n",
    "                emb_dim=embed_dim, \n",
    "                loss_id=expID, \n",
    "                ln_rate=lr, \n",
    "                batch=batch_size, \n",
    "                epc=epochs, \n",
    "                ls_mrgn=loss_margin, \n",
    "                trn_sz=train_data.shape[0],\n",
    "                mdl_nm=model.modelName,\n",
    "                rslt=r1)\n",
    "\n",
    "\n",
    "    print(r1) \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fahimul/.conda/envs/CVGL/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/fahimul/.conda/envs/CVGL/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Hyperparameter info:\n",
      "Exp ID: 6401805\n",
      "Embedded dimension: 1000\n",
      "Learning rate: 1e-05\n",
      "Batch Size: 99\n",
      "Loss Margin: 1\n",
      "Epoch: 50\n",
      "Training Size: 10658\n",
      "Model Name: ResNet18\n",
      "\n",
      "\n",
      "\n",
      "Extract Features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:59<00:00,  6.00it/s]\n",
      "100%|██████████| 359/359 [04:09<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Scores:\n",
      "Percentage-top1:67.85432849262637, top5:84.80806033997523, top10:90.04559270516718, top1%:99.91275469998874, time:3.057422399520874\n",
      "[67.85432849 84.80806034 90.04559271 99.9127547 ]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# from torch.cuda.amp import autocast\n",
    "import numpy as np\n",
    "from CVUSA_dataset import CVUSA_dataset_cropped, CVUSA_Dataset_Eval\n",
    "# from CVUSA_dataset import CVUSA_Dataset_Eval\n",
    "from custom_models import ResNet, VIT, CLIP_model\n",
    "from losses import Contrastive_loss, SoftTripletBiLoss, InfoNCE\n",
    "from train import train\n",
    "from eval import predict, accuracy, calculate_scores\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "from pytorch_metric_learning import losses as LS\n",
    "from helper_func import get_rand_id, hyparam_info, save_losses\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_path = '/media/fahimul/2B721C03261BDC8D/Research/datasets/CVUSA' #don't include the / at the end\n",
    "# data_path = '/home/fa947945/datasets/CVUSA_Cropped/CVUSA' #don't include the / at the end\n",
    "data_path = '/data/Research/Dataset/CVUSA_Cropped/CVUSA' #don't include the / at the end\n",
    "\n",
    "# train_data= pd.read_csv(f'{data_path}/splits/train-19zl.csv')\n",
    "train_data= pd.read_csv(f'{data_path}/splits/train-19zl_30.csv')\n",
    "val_data= pd.read_csv(f'{data_path}/splits/val-19zl.csv')\n",
    "\n",
    "# df_loss = pd.DataFrame(columns=['Loss'])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "train_ds = CVUSA_dataset_cropped(df = train_data, path=data_path, transform=transform)\n",
    "val_que = CVUSA_Dataset_Eval(data_folder=data_path, split='train', img_type='query', transforms=transform)\n",
    "val_ref = CVUSA_Dataset_Eval(data_folder=data_path, split='train', img_type='reference', transforms=transform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    embed_dim = 1000\n",
    "    lr = 0.00001\n",
    "    batch_size = 99\n",
    "    epochs = 50\n",
    "    expID = get_rand_id()\n",
    "    loss_margin = 1\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_que = DataLoader(val_que, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_ref = DataLoader(val_ref, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # os.mkdir(f'model_weights/{expID}')\n",
    "\n",
    "    # model = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model_r = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model_q = ResNet(emb_dim=embed_dim).to(device)\n",
    "\n",
    "    # model = ResNet()\n",
    "    # model = VIT().to(device)\n",
    "    # model = CLIP_model(embed_dim=embed_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # torch.save(model, f'model_weights/{expID}/model_st.pth')\n",
    "    \n",
    "    \n",
    "    m_path = f'model_weights/{6311184}/model_tr.pth'\n",
    "    model = torch.load(m_path)\n",
    "    \n",
    "\n",
    "    # criterion = TripletLoss(margin=loss_margin)\n",
    "    # criterion = nn.TripletMarginLoss(margin=0.5)\n",
    "  \n",
    "    # criterion = SoftTripletBiLoss()\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    criterion = InfoNCE(loss_function=loss_fn,\n",
    "                            device=device,\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name)\n",
    "    # optimizer = optim.Adam(parameters, lr=lr)\n",
    "    optimizer = optim.AdamW(parameters, lr=lr)\n",
    "    # optimizer = optim.SGD(parameters, lr=lr)\n",
    "\n",
    "    \n",
    "    \n",
    "    hyparam_info(emb_dim=embed_dim, loss_id=expID, ln_rate=lr, batch=batch_size, epc=epochs, ls_mrgn=loss_margin, trn_sz=train_data.shape[0], mdl_nm=model.modelName)\n",
    "\n",
    "    # print(\"Training Start\")\n",
    "    # all_loses = train(model, criterion, optimizer, train_loader, num_epochs=epochs, dev=device)\n",
    "    # df_loss = pd.DataFrame({'Loss': all_loses})\n",
    "    # df_loss.to_csv(f'losses/losses_{expID}.csv')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nExtract Features:\")\n",
    "    query_features, query_labels = predict(model=model, dataloader=val_loader_que, dev=device, isQuery=True)\n",
    "    reference_features, reference_labels = predict(model = model, dataloader=val_loader_ref, dev=device, isQuery=False) \n",
    "    \n",
    "\n",
    "\n",
    "    print(\"Compute Scores:\")\n",
    "    # r1 =  calculate_scores(query_features, reference_features, query_labels, reference_labels, step_size=1000, ranks=[1, 5, 10])\n",
    "    r1 =  accuracy(query_features=query_features, reference_features=reference_features, query_labels=query_labels, topk=[1, 5, 10])\n",
    "    \n",
    "    # save_losses(df=df_loss, \n",
    "    #             emb_dim=embed_dim, \n",
    "    #             loss_id=expID, \n",
    "    #             ln_rate=lr, \n",
    "    #             batch=batch_size, \n",
    "    #             epc=epochs, \n",
    "    #             ls_mrgn=loss_margin, \n",
    "    #             trn_sz=train_data.shape[0],\n",
    "    #             mdl_nm=model.modelName,\n",
    "    #             rslt=r1)\n",
    "\n",
    "\n",
    "    print(r1) \n",
    "        \n",
    "\n",
    "    # torch.save(model, f'model_weights/{expID}/model_tr.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fahimul/.conda/envs/CVGL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/fahimul/.conda/envs/CVGL/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/fahimul/.conda/envs/CVGL/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fahimul/.conda/envs/CVGL/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/fahimul/.conda/envs/CVGL/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter info:\n",
      "Exp ID: 6257958\n",
      "Embedded dimension: 1000\n",
      "Learning rate: 1e-05\n",
      "Batch Size: 64\n",
      "Loss Margin: 1\n",
      "Epoch: 100\n",
      "Training Size: 10658\n",
      "Model Name: CLIP\n",
      "\n",
      "\n",
      "Training Start\n",
      "\n",
      "Date: 2024-05-20 22:19:19.471950\n",
      "\n",
      "Epoch#0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:45<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Loss: 3.67783522605896\n",
      "Epoch#1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:48<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 Loss: 3.4610238075256348\n",
      "Epoch#2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:39<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 Loss: 3.390040874481201\n",
      "Epoch#3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:44<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 Loss: 3.353699207305908\n",
      "Epoch#4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:54<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 Loss: 3.3104639053344727\n",
      "Epoch#5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:45<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100 Loss: 3.2868146896362305\n",
      "Epoch#6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:54<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100 Loss: 3.261460065841675\n",
      "Epoch#7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:13<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/100 Loss: 3.2388217449188232\n",
      "Epoch#8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:05<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/100 Loss: 3.2311360836029053\n",
      "Epoch#9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:59<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100 Loss: 3.208472728729248\n",
      "Epoch#10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:21<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100 Loss: 3.1945183277130127\n",
      "Epoch#11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:58<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/100 Loss: 3.1947290897369385\n",
      "Epoch#12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:06<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/100 Loss: 3.1859302520751953\n",
      "Epoch#13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:01<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/100 Loss: 3.178889751434326\n",
      "Epoch#14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:53<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/100 Loss: 3.170306444168091\n",
      "Epoch#15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:53<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/100 Loss: 3.164330005645752\n",
      "Epoch#16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:14<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/100 Loss: 3.1545979976654053\n",
      "Epoch#17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:10<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/100 Loss: 3.146165609359741\n",
      "Epoch#18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:38<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/100 Loss: 3.1381149291992188\n",
      "Epoch#19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:12<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/100 Loss: 3.130805253982544\n",
      "Epoch#20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:12<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/100 Loss: 3.12326979637146\n",
      "Epoch#21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:08<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/100 Loss: 3.1190996170043945\n",
      "Epoch#22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:09<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/100 Loss: 3.1181888580322266\n",
      "Epoch#23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:28<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/100 Loss: 3.116948366165161\n",
      "Epoch#24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:06<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/100 Loss: 3.1094067096710205\n",
      "Epoch#25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:02<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/100 Loss: 3.1017422676086426\n",
      "Epoch#26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:26<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/100 Loss: 3.097783088684082\n",
      "Epoch#27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:38<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/100 Loss: 3.094224452972412\n",
      "Epoch#28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:51<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/100 Loss: 3.089644193649292\n",
      "Epoch#29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:12<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/100 Loss: 3.084209442138672\n",
      "Epoch#30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:57<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/100 Loss: 3.079519748687744\n",
      "Epoch#31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:49<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32/100 Loss: 3.0773210525512695\n",
      "Epoch#32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:16<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/100 Loss: 3.0771524906158447\n",
      "Epoch#33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:16<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/100 Loss: 3.0760586261749268\n",
      "Epoch#34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:11<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/100 Loss: 3.0722129344940186\n",
      "Epoch#35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:45<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36/100 Loss: 3.068732261657715\n",
      "Epoch#36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:39<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37/100 Loss: 3.06258487701416\n",
      "Epoch#37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:08<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/100 Loss: 3.058687210083008\n",
      "Epoch#38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:08<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/100 Loss: 3.055567979812622\n",
      "Epoch#39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:29<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/100 Loss: 3.053041458129883\n",
      "Epoch#40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:02<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41/100 Loss: 3.052685260772705\n",
      "Epoch#41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:38<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42/100 Loss: 3.054924249649048\n",
      "Epoch#42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:07<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/100 Loss: 3.0530338287353516\n",
      "Epoch#43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:12<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44/100 Loss: 3.0506651401519775\n",
      "Epoch#44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:12<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45/100 Loss: 3.0471460819244385\n",
      "Epoch#45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:08<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46/100 Loss: 3.042741060256958\n",
      "Epoch#46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:10<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/100 Loss: 3.037795305252075\n",
      "Epoch#47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:58<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48/100 Loss: 3.0342514514923096\n",
      "Epoch#48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [06:41<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49/100 Loss: 3.0324153900146484\n",
      "Epoch#49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:03<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50/100 Loss: 3.0312507152557373\n",
      "Epoch#50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [07:04<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51/100 Loss: 3.031346082687378\n",
      "Epoch#51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [04:42<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52/100 Loss: 3.031572103500366\n",
      "Epoch#52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53/100 Loss: 3.0308678150177\n",
      "Epoch#53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54/100 Loss: 3.02876615524292\n",
      "Epoch#54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:56<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55/100 Loss: 3.028168201446533\n",
      "Epoch#55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:56<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56/100 Loss: 3.0268430709838867\n",
      "Epoch#56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/100 Loss: 3.023682117462158\n",
      "Epoch#57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:56<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58/100 Loss: 3.0221352577209473\n",
      "Epoch#58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59/100 Loss: 3.01987886428833\n",
      "Epoch#59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60/100 Loss: 3.0188169479370117\n",
      "Epoch#60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:59<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61/100 Loss: 3.0181427001953125\n",
      "Epoch#61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62/100 Loss: 3.0171589851379395\n",
      "Epoch#62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63/100 Loss: 3.01503849029541\n",
      "Epoch#63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64/100 Loss: 3.0125269889831543\n",
      "Epoch#64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65/100 Loss: 3.011002779006958\n",
      "Epoch#65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:59<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66/100 Loss: 3.0106594562530518\n",
      "Epoch#66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67/100 Loss: 3.0104265213012695\n",
      "Epoch#67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68/100 Loss: 3.010394811630249\n",
      "Epoch#68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:59<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69/100 Loss: 3.0106945037841797\n",
      "Epoch#69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70/100 Loss: 3.011859893798828\n",
      "Epoch#70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:59<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71/100 Loss: 3.039456605911255\n",
      "Epoch#71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72/100 Loss: 3.0525519847869873\n",
      "Epoch#72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73/100 Loss: 3.032297372817993\n",
      "Epoch#73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74/100 Loss: 3.009859561920166\n",
      "Epoch#74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75/100 Loss: 3.0010905265808105\n",
      "Epoch#75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76/100 Loss: 2.997390031814575\n",
      "Epoch#76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77/100 Loss: 2.9954235553741455\n",
      "Epoch#77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78/100 Loss: 2.994933605194092\n",
      "Epoch#78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79/100 Loss: 2.99466609954834\n",
      "Epoch#79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80/100 Loss: 2.9944465160369873\n",
      "Epoch#80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/100 Loss: 2.994993209838867\n",
      "Epoch#81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [04:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82/100 Loss: 2.9961740970611572\n",
      "Epoch#82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83/100 Loss: 2.9975948333740234\n",
      "Epoch#83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84/100 Loss: 2.999452590942383\n",
      "Epoch#84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85/100 Loss: 3.0016772747039795\n",
      "Epoch#85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/100 Loss: 3.0031633377075195\n",
      "Epoch#86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87/100 Loss: 3.003349542617798\n",
      "Epoch#87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88/100 Loss: 3.0017383098602295\n",
      "Epoch#88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89/100 Loss: 2.999363422393799\n",
      "Epoch#89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90/100 Loss: 2.9977176189422607\n",
      "Epoch#90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91/100 Loss: 3.0091552734375\n",
      "Epoch#91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92/100 Loss: 3.0385544300079346\n",
      "Epoch#92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93/100 Loss: 3.0350849628448486\n",
      "Epoch#93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94/100 Loss: 3.0056509971618652\n",
      "Epoch#94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95/100 Loss: 2.9953083992004395\n",
      "Epoch#95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96/100 Loss: 2.9920146465301514\n",
      "Epoch#96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97/100 Loss: 2.9905757904052734\n",
      "Epoch#97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98/100 Loss: 2.989732503890991\n",
      "Epoch#98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99/100 Loss: 2.989344596862793\n",
      "Epoch#99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100 Loss: 2.9894866943359375\n",
      "\n",
      "Date: 2024-05-21 07:31:15.466920\n",
      "\n",
      "\n",
      "Extract Features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 556/556 [01:13<00:00,  7.59it/s]\n",
      "100%|██████████| 556/556 [04:37<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Scores:\n",
      "Percentage-top1:18.445345041089723, top5:25.450298322638748, top10:28.06484295845998, top1%:49.40898345153664, time:2.6157195568084717\n",
      "[18.44534504 25.45029832 28.06484296 49.40898345]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# from torch.cuda.amp import autocast\n",
    "import numpy as np\n",
    "from CVUSA_dataset import CVUSA_dataset_cropped, CVUSA_Dataset_Eval\n",
    "# from CVUSA_dataset import CVUSA_Dataset_Eval\n",
    "from custom_models import ResNet, VIT, CLIP_model\n",
    "from losses import Contrastive_loss, SoftTripletBiLoss, InfoNCE\n",
    "from train import train\n",
    "from eval import predict, accuracy, calculate_scores\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "from pytorch_metric_learning import losses as LS\n",
    "from helper_func import get_rand_id, hyparam_info, save_losses\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_path = '/media/fahimul/2B721C03261BDC8D/Research/datasets/CVUSA' #don't include the / at the end\n",
    "# data_path = '/home/fa947945/datasets/CVUSA_Cropped/CVUSA' #don't include the / at the end\n",
    "data_path = '/data/Research/Dataset/CVUSA_Cropped/CVUSA' #don't include the / at the end\n",
    "\n",
    "# train_data= pd.read_csv(f'{data_path}/splits/train-19zl.csv')\n",
    "train_data= pd.read_csv(f'{data_path}/splits/train-19zl_30.csv')\n",
    "val_data= pd.read_csv(f'{data_path}/splits/val-19zl.csv')\n",
    "\n",
    "# df_loss = pd.DataFrame(columns=['Loss'])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "train_ds = CVUSA_dataset_cropped(df = train_data, path=data_path, transform=transform)\n",
    "val_que = CVUSA_Dataset_Eval(data_folder=data_path, split='train', img_type='query', transforms=transform)\n",
    "val_ref = CVUSA_Dataset_Eval(data_folder=data_path, split='train', img_type='reference', transforms=transform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    embed_dim = 1000\n",
    "    lr = 0.00001\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    expID = get_rand_id()\n",
    "    loss_margin = 1\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_que = DataLoader(val_que, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_ref = DataLoader(val_ref, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # os.mkdir(f'model_weights/{expID}')\n",
    "\n",
    "    # model = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model_r = ResNet(emb_dim=embed_dim).to(device)\n",
    "    # model_q = ResNet(emb_dim=embed_dim).to(device)\n",
    "\n",
    "    # model = ResNet().to(device)\n",
    "    # model = VIT().to(device)\n",
    "    model = CLIP_model(embed_dim=embed_dim)\n",
    "    \n",
    "    # torch.save(model, f'model_weights/{expID}/model_st.pth')\n",
    "\n",
    "    # criterion = TripletLoss(margin=loss_margin)\n",
    "    # criterion = nn.TripletMarginLoss(margin=0.5)\n",
    "  \n",
    "    # criterion = SoftTripletBiLoss()\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.5)\n",
    "    criterion = InfoNCE(loss_function=loss_fn,\n",
    "                            device=device,\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name)\n",
    "    optimizer = optim.Adam(parameters, lr=lr)\n",
    "    # optimizer = optim.AdamW(parameters, lr=lr)\n",
    "    # optimizer = optim.SGD(parameters, lr=lr)\n",
    "\n",
    "    \n",
    "    \n",
    "    hyparam_info(emb_dim=embed_dim, loss_id=expID, ln_rate=lr, batch=batch_size, epc=epochs, ls_mrgn=loss_margin, trn_sz=train_data.shape[0], mdl_nm=model.modelName)\n",
    "\n",
    "    print(\"Training Start\")\n",
    "    all_loses = train(model, criterion, optimizer, train_loader, num_epochs=epochs, dev=device)\n",
    "    df_loss = pd.DataFrame({'Loss': all_loses})\n",
    "    df_loss.to_csv(f'losses/losses_{expID}.csv')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nExtract Features:\")\n",
    "    query_features, query_labels = predict(model=model, dataloader=val_loader_que, dev=device, isQuery=True)\n",
    "    reference_features, reference_labels = predict(model = model, dataloader=val_loader_ref, dev=device, isQuery=False) \n",
    "    \n",
    "\n",
    "\n",
    "    print(\"Compute Scores:\")\n",
    "    # r1 =  calculate_scores(query_features, reference_features, query_labels, reference_labels, step_size=1000, ranks=[1, 5, 10])\n",
    "    r1 =  accuracy(query_features=query_features, reference_features=reference_features, query_labels=query_labels, topk=[1, 5, 10])\n",
    "    \n",
    "    save_losses(df=df_loss, \n",
    "                emb_dim=embed_dim, \n",
    "                loss_id=expID, \n",
    "                ln_rate=lr, \n",
    "                batch=batch_size, \n",
    "                epc=epochs, \n",
    "                ls_mrgn=loss_margin, \n",
    "                trn_sz=train_data.shape[0],\n",
    "                mdl_nm=model.modelName,\n",
    "                rslt=r1)\n",
    "\n",
    "\n",
    "    print(r1) \n",
    "        \n",
    "\n",
    "    # torch.save(model, f'model_weights/{expID}/model_tr.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
